{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polish companies bankruptcy prediction\n",
    "### https://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is about bankruptcy prediction of Polish companies. The data was collected from Emerging Markets Information Service (EMIS, [Web Link]), which is a database containing information on emerging markets around the world. The bankrupt companies were analyzed in the period 2000-2012, while the still operating companies were evaluated from 2007 to 2013.<br><br>\n",
    "The dataset is splitted into 5 different files, which are related to forecast period for bancruptcy prediction. Each dataset contains 64 attributes. Some values are missing and needs to be handled.<br><br>\n",
    "I find this case very interesting to look closer. I will try to determine which classification model suits best to this dataset, pick best parameters. I will also try to see if there could be the same model fit to all files.<br><br>\n",
    "For preduction I will evaluate followings classification models:\n",
    "* Logistic Regression\n",
    "* KNN K-Nearest Neighbors\n",
    "* Support Vector Machines (Linear & Kernel)\n",
    "* Naive Bayes\n",
    "* Decission Tree\n",
    "* Random Forest\n",
    "* Neural Networks\n",
    "\n",
    "Also each algorythm will be checked based on crossvalidation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Data Proccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting data from external files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "headers = pd.read_csv('data/headers.txt', nrows=0, sep=',').columns.tolist()\n",
    "\n",
    "raw_files = [\n",
    "    'data/1year.arff',\n",
    "    'data/2year.arff',\n",
    "    'data/3year.arff',\n",
    "    'data/4year.arff',\n",
    "    'data/5year.arff'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating some useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "\n",
    "def CreateDatasetFromSourceFile(inputFile):\n",
    "    return pd.read_csv(inputFile, delimiter=',', header=None, names=headers, skiprows=69, low_memory=False)\n",
    "\n",
    "def SettingNameToDataset(inputDataset, nameDataset):\n",
    "    inputDataset.name = nameDataset\n",
    "\n",
    "def IsAnyNanInDataset(dataset):\n",
    "    return dataset.isna().values.any()\n",
    "\n",
    "def EnsureValuesFloatType(dataset):\n",
    "    cols = dataset.columns\n",
    "    for col in cols:\n",
    "        dataset[col] = dataset[col].convert_objects(convert_numeric=True)\n",
    "    return dataset\n",
    "\n",
    "def FillMissingValuesInDataset(dataset):\n",
    "    if (IsAnyNanInDataset(dataset)):\n",
    "        dataset = dataset.fillna(0)\n",
    "    return dataset\n",
    "\n",
    "def RetrieveXYfromRawFile(file):\n",
    "    data = CreateDatasetFromSourceFile(file)\n",
    "    SettingNameToDataset(data, file)\n",
    "    data = EnsureValuesFloatType(data)\n",
    "    data = FillMissingValuesInDataset(data)\n",
    "    X, y = data.iloc[:,:-1].values, data.iloc[:,-1:].values\n",
    "    return X, y\n",
    "\n",
    "def PrintModelResults(y_pred, y_test, model, model_name):\n",
    "    print('Classification: {}'.format(model_name))\n",
    "    print('Finished calculation: {:%H:%M:%S}'.format(datetime.datetime.now()))\n",
    "    print('Best parameters: \\n{}\\n'.format(model))\n",
    "    print('Accuracy score: {:.3f}%\\n'.format(accuracy_score(y_pred, y_test)*100))\n",
    "    print('Confusion matrix: \\n{}\\n'.format(confusion_matrix(y_test, y_pred)))\n",
    "    print('Classification report: \\n{}'.format(classification_report(y_test, y_pred)))\n",
    "\n",
    "def CreateTestAndTrainSetsFromRawFile(file):\n",
    "    X, y = RetrieveXYfromRawFile(file)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "def ExecuteClassification(file, model_names, models, parameter_grids):\n",
    "    i = 0\n",
    "    X_train, X_test, y_train, y_test = CreateTestAndTrainSetsFromRawFile(file)\n",
    "    print('\\tProcessing file: {}'.format(file))\n",
    "    print('\\t===== ==== === == = == === ==== =====\\n')\n",
    "    for model, parameter_grid in zip(models, parameter_grids):\n",
    "        grid_search = GridSearchCV(model, parameter_grid, cv=5)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        PrintModelResults(y_pred, y_test, best_model, model_names[i])\n",
    "        print('Cross validation: {:.3f}%\\n'.format(cross_val_score(best_model, X_train,y_train, cv=10).mean()*100))\n",
    "        print('\\t----- ---- --- -- - -- --- ---- -----\\n')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Classifications Models, Pipeline & Grid Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model names to list of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names1 = [\n",
    "    'Logistic Regression',\n",
    "    'K-Nearest Neighbours',\n",
    "    'Naive Bayes',\n",
    "    'Decission Tree',\n",
    "    'Random Forest'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring models for Grid Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models1 = [\n",
    "    Pipeline([\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression())\n",
    "    ]),\n",
    "    Pipeline([\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"model\", KNeighborsClassifier())\n",
    "    ]),\n",
    "    Pipeline([\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"model\", GaussianNB()),\n",
    "    ]),\n",
    "    Pipeline([\n",
    "        (\"model\", DecisionTreeClassifier())\n",
    "    ]), \n",
    "    Pipeline([\n",
    "        (\"model\", RandomForestClassifier())\n",
    "    ])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring Grid Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grids1 = [\n",
    "    { # Logistic regression\n",
    "        \"scale__with_mean\": [True, False], \n",
    "        \"model__penalty\": [\"l1\", \"l2\"], \n",
    "        \"model__C\": [0.01, 0.1, 1], \n",
    "        \"model__solver\": ['liblinear', 'saga']\n",
    "    },\n",
    "    { # KNN\n",
    "        \"model__n_neighbors\": [3, 5, 7, 9, 11, 13], \n",
    "        \"model__metric\": ['minkowski'], \n",
    "        \"model__p\": [2]}, # KNN\n",
    "    { # Naive Bayes, no parameters\n",
    "    },\n",
    "    { # Decission Tree\n",
    "        \"model__min_samples_leaf\": [5, 10, 15, 20, 25, 30, 50, 70, 100], \n",
    "        \"model__max_depth\": [5, 10, 15, 20]\n",
    "    },\n",
    "    { # Random Forests\n",
    "        \"model__n_estimators\": [10, 50, 100, 300, 500], \n",
    "        \"model__min_samples_leaf\": [5, 10, 15, 20, 25]\n",
    "    }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute classification models and report results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tProcessing file: data/1year.arff\n",
      "\t===== ==== === == = == === ==== =====\n",
      "\n",
      "Classification: Logistic Regression\n",
      "Finished calculation: 13:07:03\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=False, with_std=True)), ('model', LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=None, solver='saga',\n",
      "          tol=0.0001, verbose=0, warm_start=False))])\n",
      "\n",
      "Accuracy score: 96.088%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1351    0]\n",
      " [  55    0]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1351\n",
      "           1       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1406\n",
      "   macro avg       0.48      0.50      0.49      1406\n",
      "weighted avg       0.92      0.96      0.94      1406\n",
      "\n",
      "Cross validation: 96.157%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: K-Nearest Neighbours\n",
      "Finished calculation: 13:08:45\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=13, p=2,\n",
      "           weights='uniform'))])\n",
      "\n",
      "Accuracy score: 96.088%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1351    0]\n",
      " [  55    0]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1351\n",
      "           1       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1406\n",
      "   macro avg       0.48      0.50      0.49      1406\n",
      "weighted avg       0.92      0.96      0.94      1406\n",
      "\n",
      "Cross validation: 96.122%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: Naive Bayes\n",
      "Finished calculation: 13:08:49\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', GaussianNB(priors=None, var_smoothing=1e-09))])\n",
      "\n",
      "Accuracy score: 8.890%\n",
      "\n",
      "Confusion matrix: \n",
      "[[  73 1278]\n",
      " [   3   52]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.05      0.10      1351\n",
      "           1       0.04      0.95      0.08        55\n",
      "\n",
      "   micro avg       0.09      0.09      0.09      1406\n",
      "   macro avg       0.50      0.50      0.09      1406\n",
      "weighted avg       0.92      0.09      0.10      1406\n",
      "\n",
      "Cross validation: 8.645%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: Decission Tree\n",
      "Finished calculation: 13:09:26\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('model', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=15, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "\n",
      "Accuracy score: 97.155%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1349    2]\n",
      " [  38   17]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      1351\n",
      "           1       0.89      0.31      0.46        55\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1406\n",
      "   macro avg       0.93      0.65      0.72      1406\n",
      "weighted avg       0.97      0.97      0.96      1406\n",
      "\n",
      "Cross validation: 97.758%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: Random Forest\n",
      "Finished calculation: 13:18:23\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('model', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "\n",
      "Accuracy score: 96.515%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1351    0]\n",
      " [  49    6]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1351\n",
      "           1       1.00      0.11      0.20        55\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1406\n",
      "   macro avg       0.98      0.55      0.59      1406\n",
      "weighted avg       0.97      0.97      0.95      1406\n",
      "\n",
      "Cross validation: 96.336%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "\tProcessing file: data/2year.arff\n",
      "\t===== ==== === == = == === ==== =====\n",
      "\n",
      "Classification: Logistic Regression\n",
      "Finished calculation: 13:19:31\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False))])\n",
      "\n",
      "Accuracy score: 96.118%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1956    0]\n",
      " [  79    0]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1956\n",
      "           1       0.00      0.00      0.00        79\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2035\n",
      "   macro avg       0.48      0.50      0.49      2035\n",
      "weighted avg       0.92      0.96      0.94      2035\n",
      "\n",
      "Cross validation: 96.056%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: K-Nearest Neighbours\n",
      "Finished calculation: 13:22:36\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=11, p=2,\n",
      "           weights='uniform'))])\n",
      "\n",
      "Accuracy score: 96.118%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1956    0]\n",
      " [  79    0]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1956\n",
      "           1       0.00      0.00      0.00        79\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2035\n",
      "   macro avg       0.48      0.50      0.49      2035\n",
      "weighted avg       0.92      0.96      0.94      2035\n",
      "\n",
      "Cross validation: 96.056%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: Naive Bayes\n",
      "Finished calculation: 13:22:43\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', GaussianNB(priors=None, var_smoothing=1e-09))])\n",
      "\n",
      "Accuracy score: 8.256%\n",
      "\n",
      "Confusion matrix: \n",
      "[[  91 1865]\n",
      " [   2   77]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.05      0.09      1956\n",
      "           1       0.04      0.97      0.08        79\n",
      "\n",
      "   micro avg       0.08      0.08      0.08      2035\n",
      "   macro avg       0.51      0.51      0.08      2035\n",
      "weighted avg       0.94      0.08      0.09      2035\n",
      "\n",
      "Cross validation: 8.294%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: Decission Tree\n",
      "Finished calculation: 13:23:41\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('model', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=25, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "\n",
      "Accuracy score: 96.806%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1949    7]\n",
      " [  58   21]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      1956\n",
      "           1       0.75      0.27      0.39        79\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2035\n",
      "   macro avg       0.86      0.63      0.69      2035\n",
      "weighted avg       0.96      0.97      0.96      2035\n",
      "\n",
      "Cross validation: 96.817%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: Random Forest\n",
      "Finished calculation: 13:37:39\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('model', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=5, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "\n",
      "Accuracy score: 96.265%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1956    0]\n",
      " [  76    3]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1956\n",
      "           1       1.00      0.04      0.07        79\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2035\n",
      "   macro avg       0.98      0.52      0.53      2035\n",
      "weighted avg       0.96      0.96      0.95      2035\n",
      "\n",
      "Cross validation: 96.117%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "\tProcessing file: data/3year.arff\n",
      "\t===== ==== === == = == === ==== =====\n",
      "\n",
      "Classification: Logistic Regression\n",
      "Finished calculation: 13:39:23\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False))])\n",
      "\n",
      "Accuracy score: 95.478%\n",
      "\n",
      "Confusion matrix: \n",
      "[[2006    0]\n",
      " [  95    0]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      2006\n",
      "           1       0.00      0.00      0.00        95\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2101\n",
      "   macro avg       0.48      0.50      0.49      2101\n",
      "weighted avg       0.91      0.95      0.93      2101\n",
      "\n",
      "Cross validation: 95.239%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: K-Nearest Neighbours\n",
      "Finished calculation: 13:42:36\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=13, p=2,\n",
      "           weights='uniform'))])\n",
      "\n",
      "Accuracy score: 95.478%\n",
      "\n",
      "Confusion matrix: \n",
      "[[2006    0]\n",
      " [  95    0]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      2006\n",
      "           1       0.00      0.00      0.00        95\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2101\n",
      "   macro avg       0.48      0.50      0.49      2101\n",
      "weighted avg       0.91      0.95      0.93      2101\n",
      "\n",
      "Cross validation: 95.239%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: Naive Bayes\n",
      "Finished calculation: 13:42:45\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', GaussianNB(priors=None, var_smoothing=1e-09))])\n",
      "\n",
      "Accuracy score: 9.948%\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 121 1885]\n",
      " [   7   88]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.06      0.11      2006\n",
      "           1       0.04      0.93      0.09        95\n",
      "\n",
      "   micro avg       0.10      0.10      0.10      2101\n",
      "   macro avg       0.49      0.49      0.10      2101\n",
      "weighted avg       0.90      0.10      0.11      2101\n",
      "\n",
      "Cross validation: 9.736%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: Decission Tree\n",
      "Finished calculation: 13:44:39\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('model', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=15,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=30, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "\n",
      "Accuracy score: 96.097%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1996   10]\n",
      " [  72   23]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      2006\n",
      "           1       0.70      0.24      0.36        95\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2101\n",
      "   macro avg       0.83      0.62      0.67      2101\n",
      "weighted avg       0.95      0.96      0.95      2101\n",
      "\n",
      "Cross validation: 95.632%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: Random Forest\n",
      "Finished calculation: 14:02:24\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('model', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=15, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "\n",
      "Accuracy score: 95.478%\n",
      "\n",
      "Confusion matrix: \n",
      "[[2006    0]\n",
      " [  95    0]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      2006\n",
      "           1       0.00      0.00      0.00        95\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2101\n",
      "   macro avg       0.48      0.50      0.49      2101\n",
      "weighted avg       0.91      0.95      0.93      2101\n",
      "\n",
      "Cross validation: 95.263%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "\tProcessing file: data/4year.arff\n",
      "\t===== ==== === == = == === ==== =====\n",
      "\n",
      "Classification: Logistic Regression\n",
      "Finished calculation: 14:03:46\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=False, with_std=True)), ('model', LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=None, solver='saga',\n",
      "          tol=0.0001, verbose=0, warm_start=False))])\n",
      "\n",
      "Accuracy score: 94.487%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1851    0]\n",
      " [ 108    0]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      1851\n",
      "           1       0.00      0.00      0.00       108\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1959\n",
      "   macro avg       0.47      0.50      0.49      1959\n",
      "weighted avg       0.89      0.94      0.92      1959\n",
      "\n",
      "Cross validation: 94.766%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: K-Nearest Neighbours\n",
      "Finished calculation: 14:06:30\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=13, p=2,\n",
      "           weights='uniform'))])\n",
      "\n",
      "Accuracy score: 94.487%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1851    0]\n",
      " [ 108    0]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      1851\n",
      "           1       0.00      0.00      0.00       108\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1959\n",
      "   macro avg       0.47      0.50      0.49      1959\n",
      "weighted avg       0.89      0.94      0.92      1959\n",
      "\n",
      "Cross validation: 94.817%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: Naive Bayes\n",
      "Finished calculation: 14:06:36\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', GaussianNB(priors=None, var_smoothing=1e-09))])\n",
      "\n",
      "Accuracy score: 12.047%\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 137 1714]\n",
      " [   9   99]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.07      0.14      1851\n",
      "           1       0.05      0.92      0.10       108\n",
      "\n",
      "   micro avg       0.12      0.12      0.12      1959\n",
      "   macro avg       0.50      0.50      0.12      1959\n",
      "weighted avg       0.89      0.12      0.14      1959\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation: 11.133%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: Decission Tree\n",
      "Finished calculation: 14:07:43\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('model', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=30, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "\n",
      "Accuracy score: 95.814%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1839   12]\n",
      " [  70   38]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98      1851\n",
      "           1       0.76      0.35      0.48       108\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1959\n",
      "   macro avg       0.86      0.67      0.73      1959\n",
      "weighted avg       0.95      0.96      0.95      1959\n",
      "\n",
      "Cross validation: 95.774%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: Random Forest\n",
      "Finished calculation: 14:21:43\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('model', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=5, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "\n",
      "Accuracy score: 95.253%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1850    1]\n",
      " [  92   16]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      1851\n",
      "           1       0.94      0.15      0.26       108\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1959\n",
      "   macro avg       0.95      0.57      0.62      1959\n",
      "weighted avg       0.95      0.95      0.94      1959\n",
      "\n",
      "Cross validation: 95.544%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "\tProcessing file: data/5year.arff\n",
      "\t===== ==== === == = == === ==== =====\n",
      "\n",
      "Classification: Logistic Regression\n",
      "Finished calculation: 14:24:38\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=False, with_std=True)), ('model', LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False))])\n",
      "\n",
      "Accuracy score: 92.893%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1095    6]\n",
      " [  78    3]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      1101\n",
      "           1       0.33      0.04      0.07        81\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1182\n",
      "   macro avg       0.63      0.52      0.51      1182\n",
      "weighted avg       0.89      0.93      0.90      1182\n",
      "\n",
      "Cross validation: 93.232%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: K-Nearest Neighbours\n",
      "Finished calculation: 14:25:47\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=11, p=2,\n",
      "           weights='uniform'))])\n",
      "\n",
      "Accuracy score: 93.570%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1098    3]\n",
      " [  73    8]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      1101\n",
      "           1       0.73      0.10      0.17        81\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1182\n",
      "   macro avg       0.83      0.55      0.57      1182\n",
      "weighted avg       0.92      0.94      0.91      1182\n",
      "\n",
      "Cross validation: 93.211%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: Naive Bayes\n",
      "Finished calculation: 14:25:50\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', GaussianNB(priors=None, var_smoothing=1e-09))])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zbigniew\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 90.525%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1055   46]\n",
      " [  66   15]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      1101\n",
      "           1       0.25      0.19      0.21        81\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1182\n",
      "   macro avg       0.59      0.57      0.58      1182\n",
      "weighted avg       0.89      0.91      0.90      1182\n",
      "\n",
      "Cross validation: 77.750%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: Decission Tree\n",
      "Finished calculation: 14:26:29\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('model', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=15, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "\n",
      "Accuracy score: 95.770%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1090   11]\n",
      " [  39   42]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1101\n",
      "           1       0.79      0.52      0.63        81\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1182\n",
      "   macro avg       0.88      0.75      0.80      1182\n",
      "weighted avg       0.95      0.96      0.95      1182\n",
      "\n",
      "Cross validation: 94.818%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "Classification: Random Forest\n",
      "Finished calculation: 14:35:12\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('model', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=5, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "\n",
      "Accuracy score: 95.008%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1097    4]\n",
      " [  55   26]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      1101\n",
      "           1       0.87      0.32      0.47        81\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1182\n",
      "   macro avg       0.91      0.66      0.72      1182\n",
      "weighted avg       0.95      0.95      0.94      1182\n",
      "\n",
      "Cross validation: 94.903%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for raw_file in raw_files:\n",
    "    ExecuteClassification(raw_file, model_names1, models1, parameter_grids1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supported Vector Machine (linear and kernel). Time consuming stage thus separated from the main search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names2 = [\n",
    "    'SVM (Linear or Kernel)'\n",
    "]\n",
    "\n",
    "models2 = [\n",
    "    Pipeline([\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"model\", SVC())\n",
    "    ])\n",
    "    ]\n",
    "\n",
    "parameter_grids2 = [\n",
    "    [ # SVM\n",
    "        {\n",
    "            \"model__C\":[0.001, 0.01, 0.1, 1], \n",
    "            \"model__kernel\": [\"linear\"]\n",
    "        },\n",
    "        {\n",
    "            \"model__C\":[0.001, 0.01, 0.1], \n",
    "            \"model__kernel\": [\"rbf\"], \n",
    "            \"model__gamma\": [\"auto\", 0.001, 0.005, 0.01, 0.02],\n",
    "            \"model__max_iter\": [100],\n",
    "            \"model__tol\": [0.00002]\n",
    "        }\n",
    "    ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tProcessing file: data/1year.arff\n",
      "\t===== ==== === == = == === ==== =====\n",
      "\n",
      "Classification: SVM (Linear or Kernel)\n",
      "Finished calculation: 08:13:39\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False))])\n",
      "\n",
      "Accuracy score: 96.088%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1351    0]\n",
      " [  55    0]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1351\n",
      "           1       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1406\n",
      "   macro avg       0.48      0.50      0.49      1406\n",
      "weighted avg       0.92      0.96      0.94      1406\n",
      "\n",
      "Cross validation: 96.140%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "\tProcessing file: data/2year.arff\n",
      "\t===== ==== === == = == === ==== =====\n",
      "\n",
      "Classification: SVM (Linear or Kernel)\n",
      "Finished calculation: 08:22:34\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False))])\n",
      "\n",
      "Accuracy score: 96.118%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1956    0]\n",
      " [  79    0]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1956\n",
      "           1       0.00      0.00      0.00        79\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2035\n",
      "   macro avg       0.48      0.50      0.49      2035\n",
      "weighted avg       0.92      0.96      0.94      2035\n",
      "\n",
      "Cross validation: 96.031%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "\tProcessing file: data/3year.arff\n",
      "\t===== ==== === == = == === ==== =====\n",
      "\n",
      "Classification: SVM (Linear or Kernel)\n",
      "Finished calculation: 08:35:46\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False))])\n",
      "\n",
      "Accuracy score: 95.478%\n",
      "\n",
      "Confusion matrix: \n",
      "[[2006    0]\n",
      " [  95    0]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      2006\n",
      "           1       0.00      0.00      0.00        95\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2101\n",
      "   macro avg       0.48      0.50      0.49      2101\n",
      "weighted avg       0.91      0.95      0.93      2101\n",
      "\n",
      "Cross validation: 95.215%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "\tProcessing file: data/4year.arff\n",
      "\t===== ==== === == = == === ==== =====\n",
      "\n",
      "Classification: SVM (Linear or Kernel)\n",
      "Finished calculation: 08:55:51\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False))])\n",
      "\n",
      "Accuracy score: 94.487%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1851    0]\n",
      " [ 108    0]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      1851\n",
      "           1       0.00      0.00      0.00       108\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1959\n",
      "   macro avg       0.47      0.50      0.49      1959\n",
      "weighted avg       0.89      0.94      0.92      1959\n",
      "\n",
      "Cross validation: 94.804%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n",
      "\tProcessing file: data/5year.arff\n",
      "\t===== ==== === == = == === ==== =====\n",
      "\n",
      "Classification: SVM (Linear or Kernel)\n",
      "Finished calculation: 08:56:36\n",
      "Best parameters: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False))])\n",
      "\n",
      "Accuracy score: 93.147%\n",
      "\n",
      "Confusion matrix: \n",
      "[[1101    0]\n",
      " [  81    0]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1101\n",
      "           1       0.00      0.00      0.00        81\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1182\n",
      "   macro avg       0.47      0.50      0.48      1182\n",
      "weighted avg       0.87      0.93      0.90      1182\n",
      "\n",
      "Cross validation: 93.084%\n",
      "\n",
      "\t----- ---- --- -- - -- --- ---- -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for raw_file in raw_files:\n",
    "    ExecuteClassification(raw_file, model_names2, models2, parameter_grids2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing various Neural Networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing network architecture: (10,)\n",
      "Accuracy score in file data/1year.arff: 95.875%\n",
      "Accuracy score in file data/2year.arff: 96.020%\n",
      "Accuracy score in file data/3year.arff: 95.431%\n",
      "Accuracy score in file data/4year.arff: 94.793%\n",
      "Accuracy score in file data/5year.arff: 94.078%\n",
      "\n",
      "Testing network architecture: (20,)\n",
      "Accuracy score in file data/1year.arff: 96.017%\n",
      "Accuracy score in file data/2year.arff: 95.921%\n",
      "Accuracy score in file data/3year.arff: 95.383%\n",
      "Accuracy score in file data/4year.arff: 94.589%\n",
      "Accuracy score in file data/5year.arff: 94.332%\n",
      "\n",
      "Testing network architecture: (30,)\n",
      "Accuracy score in file data/1year.arff: 95.661%\n",
      "Accuracy score in file data/2year.arff: 95.872%\n",
      "Accuracy score in file data/3year.arff: 95.098%\n",
      "Accuracy score in file data/4year.arff: 94.538%\n",
      "Accuracy score in file data/5year.arff: 94.247%\n",
      "\n",
      "Testing network architecture: (40,)\n",
      "Accuracy score in file data/1year.arff: 95.804%\n",
      "Accuracy score in file data/2year.arff: 95.971%\n",
      "Accuracy score in file data/3year.arff: 95.145%\n",
      "Accuracy score in file data/4year.arff: 94.997%\n",
      "Accuracy score in file data/5year.arff: 94.247%\n",
      "\n",
      "Testing network architecture: (50,)\n",
      "Accuracy score in file data/1year.arff: 95.519%\n",
      "Accuracy score in file data/2year.arff: 95.823%\n",
      "Accuracy score in file data/3year.arff: 95.145%\n",
      "Accuracy score in file data/4year.arff: 94.895%\n",
      "Accuracy score in file data/5year.arff: 94.078%\n",
      "\n",
      "Testing network architecture: (70,)\n",
      "Accuracy score in file data/1year.arff: 95.590%\n",
      "Accuracy score in file data/2year.arff: 95.725%\n",
      "Accuracy score in file data/3year.arff: 95.383%\n",
      "Accuracy score in file data/4year.arff: 94.844%\n",
      "Accuracy score in file data/5year.arff: 93.909%\n",
      "\n",
      "Testing network architecture: (100,)\n",
      "Accuracy score in file data/1year.arff: 95.590%\n",
      "Accuracy score in file data/2year.arff: 95.823%\n",
      "Accuracy score in file data/3year.arff: 94.955%\n",
      "Accuracy score in file data/4year.arff: 94.844%\n",
      "Accuracy score in file data/5year.arff: 94.247%\n",
      "\n",
      "Testing network architecture: (10, 10)\n",
      "Accuracy score in file data/1year.arff: 95.661%\n",
      "Accuracy score in file data/2year.arff: 95.823%\n",
      "Accuracy score in file data/3year.arff: 95.145%\n",
      "Accuracy score in file data/4year.arff: 94.232%\n",
      "Accuracy score in file data/5year.arff: 94.162%\n",
      "\n",
      "Testing network architecture: (20, 20)\n",
      "Accuracy score in file data/1year.arff: 95.590%\n",
      "Accuracy score in file data/2year.arff: 95.921%\n",
      "Accuracy score in file data/3year.arff: 94.622%\n",
      "Accuracy score in file data/4year.arff: 94.844%\n",
      "Accuracy score in file data/5year.arff: 93.739%\n",
      "\n",
      "Testing network architecture: (30, 30)\n",
      "Accuracy score in file data/1year.arff: 95.661%\n",
      "Accuracy score in file data/2year.arff: 95.971%\n",
      "Accuracy score in file data/3year.arff: 94.907%\n",
      "Accuracy score in file data/4year.arff: 94.589%\n",
      "Accuracy score in file data/5year.arff: 92.640%\n",
      "\n",
      "Testing network architecture: (40, 40)\n",
      "Accuracy score in file data/1year.arff: 95.519%\n",
      "Accuracy score in file data/2year.arff: 95.430%\n",
      "Accuracy score in file data/3year.arff: 94.574%\n",
      "Accuracy score in file data/4year.arff: 94.385%\n",
      "Accuracy score in file data/5year.arff: 93.993%\n",
      "\n",
      "Testing network architecture: (50, 50)\n",
      "Accuracy score in file data/1year.arff: 95.804%\n",
      "Accuracy score in file data/2year.arff: 95.381%\n",
      "Accuracy score in file data/3year.arff: 95.288%\n",
      "Accuracy score in file data/4year.arff: 94.895%\n",
      "Accuracy score in file data/5year.arff: 92.724%\n",
      "\n",
      "Testing network architecture: (70, 70)\n",
      "Accuracy score in file data/1year.arff: 95.946%\n",
      "Accuracy score in file data/2year.arff: 95.627%\n",
      "Accuracy score in file data/3year.arff: 94.574%\n",
      "Accuracy score in file data/4year.arff: 94.283%\n",
      "Accuracy score in file data/5year.arff: 93.739%\n",
      "\n",
      "Testing network architecture: (100, 100)\n",
      "Accuracy score in file data/1year.arff: 95.804%\n",
      "Accuracy score in file data/2year.arff: 95.332%\n",
      "Accuracy score in file data/3year.arff: 94.574%\n",
      "Accuracy score in file data/4year.arff: 94.640%\n",
      "Accuracy score in file data/5year.arff: 93.316%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "networks = [(10,),(20,),(30,),(40,),(50,),(70,),(100,),\n",
    "            (10,10,),(20,20,),(30,30,),(40,40,),(50,50,),(70,70,),(100,100,)]\n",
    "\n",
    "for network in networks:\n",
    "    print('Testing network architecture: {}'.format(network))\n",
    "    for raw_file in raw_files:\n",
    "        X_train, X_test, y_train, y_test = CreateTestAndTrainSetsFromRawFile(raw_file)\n",
    "        model = Pipeline([\n",
    "            (\"standarization\",StandardScaler()),\n",
    "            (\"NeuralNetwork\",MLPClassifier(network,alpha=0,max_iter=1000))\n",
    "        ])\n",
    "        model.fit(X_train, y_train)\n",
    "        print('Accuracy score in file {}: {:.3f}%'.format(raw_file, accuracy_score(model.predict(X_test), y_test)*100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the best neural network is single layer with 20 neurons but cannot beat Decission Tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classifications models bring decent results with prediction score at the level of 94-97%. Classification models works great even with default parameters and optimization boosts resuts by particles of percentage.<br><br>\n",
    "Factor which distinguish especially one model from the others is the False Positive on Confusion Matrix. The lowest value can be observed on Decission Tree model and this is why Decission Tree model is more favour than others. The model did a least number of signifficant mistakes. Also looking at accuracy value across all models, Decission Tree model is better than other models by at average 1%.\n",
    "We can notice Baive Bayes is the worst model for these datasets.<br><br>\n",
    "Decission Tree is the choice in bankruptcy prediction in polish companies for different forecast period. Hovewer we still need to optimize parameters for decission tree for each period in order to get satissfied results - there is no common model for all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
